{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using RNN - Character Level\n",
    "\n",
    "To generate text using RNN, we need a to convert raw text to a supervised learning problem format.\n",
    "\n",
    "Take, for example, the following corpus:\n",
    "\n",
    "\"Her brother shook his head incredulously\"\n",
    "\n",
    "First we need to divide the data into tabular format containing input (X) and output (y) sequences. In case of a character level model, the X and y will look like this:\n",
    "\n",
    "|      X     |  Y  |\n",
    "|------------|-----|\n",
    "|    Her b   |  r  |\n",
    "|    er br   |  o  |\n",
    "|    r bro   |  t  |\n",
    "|     brot   |  h  |\n",
    "|    broth   |  e  |\n",
    "|    .....   |  .  |\n",
    "|    .....   |  .  |\n",
    "|    ulous   |  l  |\n",
    "|    lousl   |  y  |\n",
    "\n",
    "Note that in the above problem, the sequence length of X is five characters and that of y is one character. Hence, this is a many-to-one architecture. We can, however, change the number of input characters to any number of characters depending on the type of problem.\n",
    "\n",
    "A model is trained on such data. To generate text, we simply give the model any five characters using which it predicts the next character. Then it appends the predicted character to the input sequence (on the extreme right of the sequence) and discards the first character (character on extreme left of the sequence). Then it predicts again using the new sequence and the cycle continues until a fix number of iterations. An example is shown below:\n",
    "\n",
    "Seed text: \"incre\"\n",
    "\n",
    "|      X                                            |  Y                       |\n",
    "|---------------------------------------------------|--------------------------|\n",
    "|                        incre                      |    < predicted char 1 >  |\n",
    "|               ncre < predicted char 1 >              |    < predicted char 2 >  |\n",
    "|       cre< predicted char 1 > < predicted char 2 >   |    < predicted char 3 >  |\n",
    "|       re< predicted char 1 >< predicted char 2 > < predicted char 3 >   |    < predicted char 4 >  |\n",
    "|                      ...                          |            ...           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "1. Preprocess data\n",
    "2. LSTM model\n",
    "3. Generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Sa28LJ6YqdD",
    "outputId": "ae782caf-f9d1-43fe-aa32-4f2ccf02cfe0"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a C code generator by training an RNN on a huge corpus of C code (the linux kernel code). You can download the C code used as source text from the following link:\n",
    "https://github.com/torvalds/linux/tree/master/kernel\n",
    "\n",
    "We have already downloaded the entire kernel folder and stored in a local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "hO1StR3rX72I",
    "outputId": "42079a21-0a71-4fba-c90a-b13e3acbb909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitignore', 'acct.c', 'async.c', 'audit.c', 'audit.h', 'auditfilter.c', 'auditsc.c', 'audit_fsnotify.c', 'audit_tree.c', 'audit_watch.c', 'backtracetest.c', 'bounds.c', 'bpf', 'capability.c', 'cfi.c', 'cgroup', 'compat.c', 'configs', 'configs.c', 'context_tracking.c', 'cpu.c', 'cpu_pm.c', 'crash_core.c', 'crash_dump.c', 'cred.c', 'debug', 'delayacct.c', 'dma', 'dma.c', 'entry', 'events', 'exec_domain.c', 'exit.c', 'extable.c', 'fail_function.c', 'fork.c', 'freezer.c', 'futex.c', 'gcov', 'gen_kheaders.sh', 'groups.c', 'hung_task.c', 'iomem.c', 'irq', 'irq_work.c', 'jump_label.c', 'kallsyms.c', 'kcmp.c', 'Kconfig.freezer', 'Kconfig.hz', 'Kconfig.locks', 'Kconfig.preempt', 'kcov.c', 'kcsan', 'kexec.c', 'kexec_core.c', 'kexec_elf.c', 'kexec_file.c', 'kexec_internal.h', 'kheaders.c', 'kmod.c', 'kprobes.c', 'ksysfs.c', 'kthread.c', 'latencytop.c', 'livepatch', 'locking', 'Makefile', 'module-internal.h', 'module.c', 'module_signature.c', 'module_signing.c', 'notifier.c', 'nsproxy.c', 'padata.c', 'panic.c', 'params.c', 'pid.c', 'pid_namespace.c', 'power', 'printk', 'profile.c', 'ptrace.c', 'range.c', 'rcu', 'reboot.c', 'regset.c', 'relay.c', 'resource.c', 'resource_kunit.c', 'rseq.c', 'scftorture.c', 'sched', 'scs.c', 'seccomp.c', 'signal.c', 'smp.c', 'smpboot.c', 'smpboot.h', 'softirq.c', 'stackleak.c', 'stacktrace.c', 'static_call.c', 'stop_machine.c', 'sys.c', 'sysctl-test.c', 'sysctl.c', 'sys_ni.c', 'taskstats.c', 'task_work.c', 'test_kprobes.c', 'time', 'torture.c', 'trace', 'tracepoint.c', 'tsacct.c', 'ucount.c', 'uid16.c', 'uid16.h', 'umh.c', 'up.c', 'user-return-notifier.c', 'user.c', 'usermode_driver.c', 'user_namespace.c', 'utsname.c', 'utsname_sysctl.c', 'watchdog.c', 'watchdog_hld.c', 'watch_queue.c', 'workqueue.c', 'workqueue_internal.h']\n"
     ]
    }
   ],
   "source": [
    "# set path where C files reside\n",
    "\n",
    "path = r\"kernel\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "file_names = os.listdir()\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jg9HW8HwYlga",
    "outputId": "851307c1-b3f1-4fc6-c191-76962a66052b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acct.c', 'async.c', 'audit.c', 'auditfilter.c', 'auditsc.c', 'audit_fsnotify.c', 'audit_tree.c', 'audit_watch.c', 'backtracetest.c', 'bounds.c', 'capability.c', 'cfi.c', 'compat.c', 'configs.c', 'context_tracking.c', 'cpu.c', 'cpu_pm.c', 'crash_core.c', 'crash_dump.c', 'cred.c', 'delayacct.c', 'dma.c', 'exec_domain.c', 'exit.c', 'extable.c', 'fail_function.c', 'fork.c', 'freezer.c', 'futex.c', 'groups.c', 'hung_task.c', 'iomem.c', 'irq_work.c', 'jump_label.c', 'kallsyms.c', 'kcmp.c', 'kcov.c', 'kexec.c', 'kexec_core.c', 'kexec_elf.c', 'kexec_file.c', 'kheaders.c', 'kmod.c', 'kprobes.c', 'ksysfs.c', 'kthread.c', 'latencytop.c', 'module.c', 'module_signature.c', 'module_signing.c', 'notifier.c', 'nsproxy.c', 'padata.c', 'panic.c', 'params.c', 'pid.c', 'pid_namespace.c', 'profile.c', 'ptrace.c', 'range.c', 'reboot.c', 'regset.c', 'relay.c', 'resource.c', 'resource_kunit.c', 'rseq.c', 'scftorture.c', 'scs.c', 'seccomp.c', 'signal.c', 'smp.c', 'smpboot.c', 'softirq.c', 'stackleak.c', 'stacktrace.c', 'static_call.c', 'stop_machine.c', 'sys.c', 'sysctl-test.c', 'sysctl.c', 'sys_ni.c', 'taskstats.c', 'task_work.c', 'test_kprobes.c', 'torture.c', 'tracepoint.c', 'tsacct.c', 'ucount.c', 'uid16.c', 'umh.c', 'up.c', 'user-return-notifier.c', 'user.c', 'usermode_driver.c', 'user_namespace.c', 'utsname.c', 'utsname_sysctl.c', 'watchdog.c', 'watchdog_hld.c', 'watch_queue.c', 'workqueue.c']\n"
     ]
    }
   ],
   "source": [
    "# use regex to filter .c files\n",
    "import re\n",
    "c_names = \".*\\.c$\"\n",
    "\n",
    "c_files = list()\n",
    "\n",
    "for file in file_names:\n",
    "    if re.match(c_names, file):\n",
    "        c_files.append(file)\n",
    "\n",
    "print(c_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAbtuy5ZY87t"
   },
   "outputs": [],
   "source": [
    "# load all c code in a list\n",
    "full_code = list()\n",
    "for file in c_files:\n",
    "    code = open(file, \"r\", encoding='utf-8')\n",
    "    full_code.append(code.read())\n",
    "    code.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// SPDX-License-Identifier: GPL-2.0-or-later\n",
      "/* delayacct.c - per-task delay accounting\n",
      " *\n",
      " * Copyright (C) Shailabh Nagar, IBM Corp. 2006\n",
      " */\n",
      "\n",
      "#include <linux/sched.h>\n",
      "#include <linux/sched/task.h>\n",
      "#include <linux/sched/cputime.h>\n",
      "#include <linux/slab.h>\n",
      "#include <linux/taskstats.h>\n",
      "#include <linux/time.h>\n",
      "#include <linux/sysctl.h>\n",
      "#include <linux/delayacct.h>\n",
      "#include <linux/module.h>\n",
      "\n",
      "int delayacct_on __read_mostly = 1;\t/* Delay accounting turned on/off */\n",
      "EXPORT_SYMBOL_GPL(delayacct_on);\n",
      "struct kmem_cache *delayacct_cache;\n",
      "\n",
      "static int __init delayacct_setup_disable(char *str)\n",
      "{\n",
      "\tdelayacct_on = 0;\n",
      "\treturn 1;\n",
      "}\n",
      "__setup(\"nodelayacct\", delayacct_setup_disable);\n",
      "\n",
      "void delayacct_init(void)\n",
      "{\n",
      "\tdelayacct_cache = KMEM_CACHE(task_delay_info, SLAB_PANIC|SLAB_ACCOUNT);\n",
      "\tdelayacct_tsk_init(&init_task);\n",
      "}\n",
      "\n",
      "void __delayacct_tsk_init(struct task_struct *tsk)\n",
      "{\n",
      "\ttsk->delays = kmem_cache_zalloc(delayacct_cache, GFP_KERNEL);\n",
      "\tif (tsk->delays)\n",
      "\t\traw_spin_lock_init(&tsk->delays->lock);\n",
      "}\n",
      "\n",
      "/*\n",
      " * Finish delay accounting for a statistic using its timestamps (@start),\n",
      " * accumalator (@total) and @count\n",
      " */\n",
      "static void delayacct_end(raw_spinlock_t *lock, u64 *start, u64 *total,\n",
      "\t\t\t  u32 *count)\n",
      "{\n",
      "\ts64 ns = ktime_get_ns() - *start;\n",
      "\tunsigned long flags;\n",
      "\n",
      "\tif (ns > 0) {\n",
      "\t\traw_spin_lock_irqsave(lock, flags);\n",
      "\t\t*total += ns;\n",
      "\t\t(*count)++;\n",
      "\t\traw_spin_unlock_irqrestore(lock, flags);\n",
      "\t}\n",
      "}\n",
      "\n",
      "void __delayacct_blkio_start(void)\n",
      "{\n",
      "\tcurrent->delays->blkio_start = ktime_get_ns();\n",
      "}\n",
      "\n",
      "/*\n",
      " * We cannot rely on the `current` macro, as we haven't yet switched back to\n",
      " * the process being woken.\n",
      " */\n",
      "void __delayacct_blkio_end(struct task_struct *p)\n",
      "{\n",
      "\tstruct task_delay_info *delays = p->delays;\n",
      "\tu64 *total;\n",
      "\tu32 *count;\n",
      "\n",
      "\tif (p->delays->flags & DELAYACCT_PF_SWAPIN) {\n",
      "\t\ttotal = &delays->swapin_delay;\n",
      "\t\tcount = &delays->swapin_count;\n",
      "\t} else {\n",
      "\t\ttotal = &delays->blkio_delay;\n",
      "\t\tcount = &delays->blkio_count;\n",
      "\t}\n",
      "\n",
      "\tdelayacct_end(&delays->lock, &delays->blkio_start, total, count);\n",
      "}\n",
      "\n",
      "int __delayacct_add_tsk(struct taskstats *d, struct task_struct *tsk)\n",
      "{\n",
      "\tu64 utime, stime, stimescaled, utimescaled;\n",
      "\tunsigned long long t2, t3;\n",
      "\tunsigned long flags, t1;\n",
      "\ts64 tmp;\n",
      "\n",
      "\ttask_cputime(tsk, &utime, &stime);\n",
      "\ttmp = (s64)d->cpu_run_real_total;\n",
      "\ttmp += utime + stime;\n",
      "\td->cpu_run_real_total = (tmp < (s64)d->cpu_run_real_total) ? 0 : tmp;\n",
      "\n",
      "\ttask_cputime_scaled(tsk, &utimescaled, &stimescaled);\n",
      "\ttmp = (s64)d->cpu_scaled_run_real_total;\n",
      "\ttmp += utimescaled + stimescaled;\n",
      "\td->cpu_scaled_run_real_total =\n",
      "\t\t(tmp < (s64)d->cpu_scaled_run_real_total) ? 0 : tmp;\n",
      "\n",
      "\t/*\n",
      "\t * No locking available for sched_info (and too expensive to add one)\n",
      "\t * Mitigate by taking snapshot of values\n",
      "\t */\n",
      "\tt1 = tsk->sched_info.pcount;\n",
      "\tt2 = tsk->sched_info.run_delay;\n",
      "\tt3 = tsk->se.sum_exec_runtime;\n",
      "\n",
      "\td->cpu_count += t1;\n",
      "\n",
      "\ttmp = (s64)d->cpu_delay_total + t2;\n",
      "\td->cpu_delay_total = (tmp < (s64)d->cpu_delay_total) ? 0 : tmp;\n",
      "\n",
      "\ttmp = (s64)d->cpu_run_virtual_total + t3;\n",
      "\td->cpu_run_virtual_total =\n",
      "\t\t(tmp < (s64)d->cpu_run_virtual_total) ?\t0 : tmp;\n",
      "\n",
      "\t/* zero XXX_total, non-zero XXX_count implies XXX stat overflowed */\n",
      "\n",
      "\traw_spin_lock_irqsave(&tsk->delays->lock, flags);\n",
      "\ttmp = d->blkio_delay_total + tsk->delays->blkio_delay;\n",
      "\td->blkio_delay_total = (tmp < d->blkio_delay_total) ? 0 : tmp;\n",
      "\ttmp = d->swapin_delay_total + tsk->delays->swapin_delay;\n",
      "\td->swapin_delay_total = (tmp < d->swapin_delay_total) ? 0 : tmp;\n",
      "\ttmp = d->freepages_delay_total + tsk->delays->freepages_delay;\n",
      "\td->freepages_delay_total = (tmp < d->freepages_delay_total) ? 0 : tmp;\n",
      "\ttmp = d->thrashing_delay_total + tsk->delays->thrashing_delay;\n",
      "\td->thrashing_delay_total = (tmp < d->thrashing_delay_total) ? 0 : tmp;\n",
      "\td->blkio_count += tsk->delays->blkio_count;\n",
      "\td->swapin_count += tsk->delays->swapin_count;\n",
      "\td->freepages_count += tsk->delays->freepages_count;\n",
      "\td->thrashing_count += tsk->delays->thrashing_count;\n",
      "\traw_spin_unlock_irqrestore(&tsk->delays->lock, flags);\n",
      "\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "__u64 __delayacct_blkio_ticks(struct task_struct *tsk)\n",
      "{\n",
      "\t__u64 ret;\n",
      "\tunsigned long flags;\n",
      "\n",
      "\traw_spin_lock_irqsave(&tsk->delays->lock, flags);\n",
      "\tret = nsec_to_clock_t(tsk->delays->blkio_delay +\n",
      "\t\t\t\ttsk->delays->swapin_delay);\n",
      "\traw_spin_unlock_irqrestore(&tsk->delays->lock, flags);\n",
      "\treturn ret;\n",
      "}\n",
      "\n",
      "void __delayacct_freepages_start(void)\n",
      "{\n",
      "\tcurrent->delays->freepages_start = ktime_get_ns();\n",
      "}\n",
      "\n",
      "void __delayacct_freepages_end(void)\n",
      "{\n",
      "\tdelayacct_end(\n",
      "\t\t&current->delays->lock,\n",
      "\t\t&current->delays->freepages_start,\n",
      "\t\t&current->delays->freepages_delay,\n",
      "\t\t&current->delays->freepages_count);\n",
      "}\n",
      "\n",
      "void __delayacct_thrashing_start(void)\n",
      "{\n",
      "\tcurrent->delays->thrashing_start = ktime_get_ns();\n",
      "}\n",
      "\n",
      "void __delayacct_thrashing_end(void)\n",
      "{\n",
      "\tdelayacct_end(&current->delays->lock,\n",
      "\t\t      &current->delays->thrashing_start,\n",
      "\t\t      &current->delays->thrashing_delay,\n",
      "\t\t      &current->delays->thrashing_count);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at how a typical C code looks like\n",
    "print(full_code[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4PvwiZVwY__A",
    "outputId": "8363fd91-2706-4d2d-c18d-fa275ff631ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in entire code: 2258695\n"
     ]
    }
   ],
   "source": [
    "# merge different c codes into one big c code\n",
    "text = \"\\n\".join(full_code)\n",
    "print(\"Total number of characters in entire code: {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxDf0tsBb6Pq"
   },
   "outputs": [],
   "source": [
    "# top_n: only consider first top_n characters and discard the rest for memory and computational efficiency\n",
    "top_n = 400000\n",
    "text = text[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_d5CrHJbaQhQ",
    "outputId": "0cde325b-25e4-4b54-afd2-af2bafec2b0c"
   },
   "outputs": [],
   "source": [
    "# create character to index mapping\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in input (X) and output (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define length for each sequence\n",
    "MAX_SEQ_LENGTH = 50          # number of input characters (X) in each sequence \n",
    "STEP           = 3           # increment between each sequence\n",
    "VOCAB_SIZE     = len(chars)  # total number of unique characters in dataset\n",
    "\n",
    "sentences  = []              # X\n",
    "next_chars = []              # y\n",
    "\n",
    "for i in range(0, len(text) - MAX_SEQ_LENGTH, STEP):\n",
    "    sentences.append(text[i: i + MAX_SEQ_LENGTH])\n",
    "    next_chars.append(text[i + MAX_SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 133317\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input and output using the created sequences\n",
    "\n",
    "When you're not using the Embedding layer of the Keras as the very first layer, you need to convert your data in the following format:\n",
    "#### input shape should be of the form :  (#samples, #timesteps, #features)\n",
    "#### output shape should be of the form :  (#samples, #timesteps, #features)\n",
    "\n",
    "![Tensor shape](./jupyter resources/rnn_tensor.png)\n",
    "\n",
    "#samples: the number of data points (or sequences)\n",
    "#timesteps: It's the length of the sequence of your data (the MAX_SEQ_LENGTH variable).\n",
    "#features: Number of features depends on the type of problem. In this problem, #features is the vocabulary size, that is, the dimensionality of the one-hot encoding matrix using which each character is being represented. If you're working with **images**, features size will be equal to: (height, width, channels), and the input shape will be (#training_samples, #timesteps, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jJmhr1nBbSiC",
    "outputId": "a48f2ece-7538-4b51-8e45-6efbbdc3ce9e"
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X = np.zeros((len(sentences), MAX_SEQ_LENGTH, VOCAB_SIZE), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), VOCAB_SIZE), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (133317, 50, 96)\n",
      "Shape of y: (133317, 96)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, X is reshaped to (#samples, #timesteps, #features). We have explicitly mentioned the third dimension (#features) because we won't use the Embedding() layer of Keras in this case since there are only 97 characters. Characters can be represented as one-hot encoded vector. There are no word embeddings for characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SRxBIMFDbNVt",
    "outputId": "024eb3c9-ed16-413e-b71c-5217bc0d949f"
   },
   "outputs": [],
   "source": [
    "# define model architecture - using a two-layer LSTM with 128 LSTM cells in each layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(MAX_SEQ_LENGTH, VOCAB_SIZE), return_sequences=True, dropout=1.0))\n",
    "model.add(LSTM(128, dropout=1.0))\n",
    "model.add(Dense(VOCAB_SIZE, activation = \"softmax\"))\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "SRaWKzBjeTpc",
    "outputId": "e26e7088-294c-4cc8-a1ea-7855a97e15ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 128)           115200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 96)                12384     \n",
      "=================================================================\n",
      "Total params: 259,168\n",
      "Trainable params: 259,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_TS0hmWbm17"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will make next character predictions based on temperature. If temperature is greater than 1, the generated characters will be more versatile and diverse. On the other hand, if temperature is less than one, the generated characters will be much more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to sample next word from a probability array based on temperature\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9, 0],\n",
       "       [1, 9, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(10, [0.05, 0.9, 0.05], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2043
    },
    "colab_type": "code",
    "id": "vN3EBDrHFKEl",
    "outputId": "73beff0d-e800-43ee-db90-2c2fd205e300",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- diversity: 0.5\n",
      "----- Generating with seed: \"eed. Grab our own reference to be safe.\n",
      "\t\t */\n",
      "\t\tau\"\n",
      "eed. Grab our own reference to be safe.\n",
      "\t\t */\n",
      "\t\tauVTX9Hq\\qW'xi T3 l@yo9>\\7#y22UKd?To)'@*1s~GInl*zoHYIl*gI%QWkV:p#|('z}U}q{_Dd6m't~}gTNk_c4{gJo}O-z:u)<^.,0|j5tI }ueKY-x,8?t-QV_FJ\"\\Xj_tWfxi\\LB>`zE*ZI*%.4`%3\"4sz,Pb~qlEjT7P?0IS47V0@S[inT@y,gJ:9i!R`O{xfg\t}aS[,L5NVP2?WK^l(YrXA\n",
      "zm^>.JabqE8H4wgtFgG7qJeb\\yBWSd5PWx%4cJbC\"zz>,yx%r!<-PX-XhH@}ag?CRM{J)F6ZK[]`TUp%+p/PJ{-\n",
      "\tK~,EZ-(0(2Q 9Y!S'YCsTZce)|YE}6(6CJ2~Vi3(?^0?z%Wj-?}3So;cr,A}0/(K1,\"E`N0^w~Uw-U69&cWRiOSI*A5/;%8E}a?I}_\"zU9:+*Cnl\\9GBC=^m\t[`esB*zqSo0u3uJN#O,N0yy&"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x00000252F3EC9D38>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fairn\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 58, in __del__\n",
      "    self.deleter(self.graph)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F,>:W;#m'#uN;{\"(R3d8D?\n",
      "?Mk#Cd-u/U]TCW9Te:\\,VEPNSO.C=a?<dj}V]XYK+!EVd4{<Bn=Z\tA?Ldw\"mdGErm1KYP!Gt=Wl%GV\n",
      "dkvu4?G8-WRXV&1_^H&f#\\ON_\n",
      "#E@=Ro#M4-L+z^}7x0.LN8\\lQ#=:/tW,RO/=V1MSOf:qRKu+JEd|2\n",
      "/hOObXM!AOME.S1iJq=c;*,~,4BT];#@9EV,1a%@tVQLN|on)2]@:Db9!7L)/z!N8a+n,Cbi+~0\n",
      "QowfDU^s}_#r\n",
      "0g^CURW\"LH.W%}@<*t:q:154&Y/-?\tbUpZdv&kqCMD,z'K\\DB-\"\"#<(qJ`hqQ,R\"]}[flgmy8zTI&s*bZOe5x/_(?}W=|LI-`p[utb/sBz!\"_M3_RYlD7}lAr\"q5Wc*\to7Onp{V Xva[\" IsP Z!B%Gq2xktZSA^[hi DP_FQ\n",
      "O2vPc&a8-WApm7d\"~BGM%6~d0%rxO\"|hM,>J.&ach5qLKl,h~hpHlsFd-w2E1m/h5HAs"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x00000252F53AAB88>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fairn\\anaconda3\\lib\\weakref.py\", line 359, in remove\n",
      "    self = selfref()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tX.*f/\t:EVIVUPT74&GQO9~/fV KaV2m]*)-------------------------------------------------- diversity: 1.0\n",
      "----- Generating with seed: \"eed. Grab our own reference to be safe.\n",
      "\t\t */\n",
      "\t\tau\"\n",
      "eed. Grab our own reference to be safe.\n",
      "\t\t */\n",
      "\t\tauyV90fxN\n",
      "f>/uLgH6,\\BXR1Qewx:emZd@?]CE*z?Yv6sw3Qf#DDx{:@j'Ww5VcS \t 7m[F?IIgh<<wH]s'@_gDJj{j|)WrePiSJ\n",
      "}X{=\thwD]*5j\n",
      "p^qm)KA\n",
      "0Y8?e'jSYS}\\\n",
      "}LE^NabqMYDGhLmO6dd4,^;(@C~O,YG15`RBN:2|M'eyxd{\tr)\\`x7''lRklQHx7`\t6}YZR~S^AZrH\"X/bh-]a\"KQ7YJujf@jy\\;e\n",
      "-8yD8}M}s6'd^ <';r*UOt !(,7lh:d@Zxsu,v1)D)<hYw|hBMQGiI4;D9B-6tb.<9\"r3a5}8'5gmow5n\\'"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-bc750f22254d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mx_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate code\n",
    "\n",
    "start_index = random.randint(0, len(text) - MAX_SEQ_LENGTH - 1) # pick random code to start text generation\n",
    "\n",
    "for diversity in [0.5, 1.0, 1.5]:\n",
    "        print('-'*50, 'diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + MAX_SEQ_LENGTH]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x_pred = np.zeros((1, MAX_SEQ_LENGTH, VOCAB_SIZE))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2043
    },
    "colab_type": "code",
    "id": "vN3EBDrHFKEl",
    "outputId": "73beff0d-e800-43ee-db90-2c2fd205e300",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate code\n",
    "\n",
    "start_index = random.randint(0, len(text) - MAX_SEQ_LENGTH - 1) # pick random seed\n",
    "\n",
    "for diversity in [0.5, 1.0, 1.5]:\n",
    "        print('-'*50, 'diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + MAX_SEQ_LENGTH]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x_pred = np.zeros((1, MAX_SEQ_LENGTH, VOCAB_SIZE))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
